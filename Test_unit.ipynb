{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb824ee-9190-473d-b373-fbab82965624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import process\n",
    "\n",
    "# Load transactions file\n",
    "transactions_path = \"C:/Users/lgino/code/Lothardill/Hospitalis/final_countdown.csv\"\n",
    "df_transactions = pd.read_csv(transactions_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Load Am√©lie file\n",
    "amelie_path = \"C:/Users/lgino/code/Lothardill/Hospitalis/OPENLLP_Produit_Fournisseur.csv\"\n",
    "df_amelie = pd.read_csv(amelie_path, encoding=\"utf-8-sig\")\n",
    "# Ensure Fournisseur column exists\n",
    "if \"Fournisseur\" not in df_transactions.columns:\n",
    "    raise ValueError(\"‚ùå 'Fournisseur' column missing in Transactions file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600bb23a-9920-47c5-b5bf-23991d30fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(article_name, choices, threshold=80):\n",
    "    \"\"\"\n",
    "    Find the best fuzzy match for `article_name` within `choices`.\n",
    "    Returns the matched name if score >= threshold, else None.\n",
    "    \"\"\"\n",
    "    match, score = process.extractOne(article_name, choices)\n",
    "    return match if score >= threshold else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c6fda0-cead-45ee-9726-123364cb8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_transactions.groupby(\"Libell√© Article\").agg({\n",
    "    \"Fournisseur\": \"first\",  # Keep first supplier per product\n",
    "    \"Commande\": \"first\",     # Keep first order reference\n",
    "    \"D√©p√¥t DMI\": \"first\",    # Keep first depot reference\n",
    "    \"R√©f. Article\": \"first\", # Keep first product reference\n",
    "    \"Quantit√©\": \"sum\",       # Sum total quantity\n",
    "    \"Prix HT\": \"mean\",       # Average unit price (before tax)\n",
    "    \"Prix TTC\": \"mean\",      # Average unit price (tax included)\n",
    "    \"Mnt HT\": \"sum\",         # Total purchase amount (before tax)\n",
    "    \"Mnt TTC\": \"sum\"         # Total purchase amount (tax included)\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250344b-a0ee-4fcb-9bcd-8fa2a16065fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 0/5953297 rows - Elapsed Time: 4.57s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Get unique product names from Am√©lie for fuzzy matching\n",
    "amelie_products = df_amelie[\"√âtiquettes de lignes\"].tolist()\n",
    "\n",
    "# Create a new column for storing matches\n",
    "df_transactions[\"Match\"] = None\n",
    "\n",
    "# Track progress efficiently\n",
    "total_rows = len(df_transactions)\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over transactions to find matches\n",
    "for index, row in df_transactions.iterrows():\n",
    "    etiquette_match = None\n",
    "\n",
    "    # 1Ô∏è‚É£ First check if \"Fournisseur\" appears in \"√âtiquettes de lignes\"\n",
    "    for etiq in amelie_products:\n",
    "        if row[\"Fournisseur\"] in etiq:\n",
    "            etiquette_match = etiq\n",
    "            break  # Stop once we find a match\n",
    "\n",
    "    # 2Ô∏è‚É£ If we found a Fournisseur match, try fuzzy matching on \"Libell√© Article\"\n",
    "    if etiquette_match:\n",
    "        best_match = find_best_match(row[\"Libell√© Article\"], amelie_products)\n",
    "        df_transactions.at[index, \"Match\"] = best_match  # Store best match\n",
    "\n",
    "    # 3Ô∏è‚É£ Lightweight progress tracking\n",
    "    if index % 500 == 0:  # Update every 500 rows\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Processed {index}/{total_rows} rows - Elapsed Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(\"üéâ Matching Completed!\")\n",
    "\n",
    "# Save the updated transactions file\n",
    "df_transactions.to_csv(\"C:/Users/lgino/code/Lothardill/Hospitalis/Transactions_with_Match.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"‚úÖ Merged file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e935dc2-65ba-4c25-8c52-8d646c4fa3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Transactions file\n",
    "transactions_path = \"C:/Users/lgino/code/Lothardill/Hospitalis/Transactions_2024.csv\"\n",
    "df_transactions = pd.read_csv(transactions_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Load Am√©lie file\n",
    "amelie_path = \"/mnt/data/OPENLLP_Produit_Fournisseur.csv\"\n",
    "df_amelie = pd.read_csv(amelie_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_amelie.columns = [\"√âtiquettes de lignes\", \"Moyenne Prix Unitaire\"]\n",
    "\n",
    "# Show sample data\n",
    "print(\"üöÄ Transactions Sample:\")\n",
    "print(df_transactions[[\"Libell√© Article\", \"Prix HT\"]].head())\n",
    "\n",
    "print(\"\\nüöÄ Am√©lie Sample:\")\n",
    "print(df_amelie.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8963f67-a0e3-42b0-ba2f-39c353650c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fournisseur filtering done! 1452 possible matches found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lgino\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'parallel_fuzzy_matching.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(pool\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mapply(match_article, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), df_split))\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 37\u001b[0m df_transactions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m parallel_fuzzy_matching(df_transactions)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 4Ô∏è‚É£ Save optimized result\u001b[39;00m\n\u001b[0;32m     40\u001b[0m df_transactions\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/lgino/code/Lothardill/Hospitalis/Transactions_with_Match.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36mparallel_fuzzy_matching\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     32\u001b[0m df_split \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(df, num_partitions)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(num_partitions) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m---> 34\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(pool\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mapply(match_article, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), df_split))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[1;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 540\u001b[0m     put(task)\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    542\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(_ForkingPickler\u001b[38;5;241m.\u001b[39mdumps(obj))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[1;34m(cls, obj, protocol)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28mcls\u001b[39m(buf, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get local object 'parallel_fuzzy_matching.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from thefuzz import process\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Load data\n",
    "transactions_path = \"C:/Users/lgino/code/Lothardill/Hospitalis/final_countdown.csv\"\n",
    "df_transactions = pd.read_csv(\"C:/Users/lgino/code/Lothardill/Hospitalis/final_countdown.csv\")\n",
    "amelie_path = \"C:/Users/lgino/code/Lothardill/Hospitalis/OPENLLP_Produit_Fournisseur.csv\"\n",
    "df_amelie = pd.read_csv(\"C:/Users/lgino/code/Lothardill/Hospitalis/OPENLLP_Produit_Fournisseur.csv\")\n",
    "\n",
    "# 1Ô∏è‚É£ Pre-filter Am√©lie based on Fournisseur to reduce search space\n",
    "fournisseur_map = {\n",
    "    row[\"√âtiquettes de lignes\"]: row[\"Moyenne de Prix unitaire (Base remboursement / Quantit√©)\"]\n",
    "    for _, row in df_amelie.iterrows()\n",
    "    if any(fournisseur in row[\"√âtiquettes de lignes\"] for fournisseur in df_transactions[\"Fournisseur\"].unique())\n",
    "}\n",
    "print(f\"‚úÖ Fournisseur filtering done! {len(fournisseur_map)} possible matches found.\")\n",
    "\n",
    "# 2Ô∏è‚É£ Fast fuzzy matching function\n",
    "def match_article(row):\n",
    "    \"\"\"Find best matching article using pre-filtered fournisseur list.\"\"\"\n",
    "    if row[\"Fournisseur\"] in fournisseur_map:\n",
    "        best_match, score = process.extractOne(row[\"Libell√© Article\"], fournisseur_map.keys())\n",
    "        return best_match if score >= 80 else None\n",
    "    return None\n",
    "\n",
    "# 3Ô∏è‚É£ Parallel processing for ultra-fast matching\n",
    "def parallel_fuzzy_matching(df):\n",
    "    \"\"\"Splits dataframe and applies fuzzy matching in parallel for massive speedup.\"\"\"\n",
    "    num_partitions = cpu_count()\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    with Pool(num_partitions) as pool:\n",
    "        df = pd.concat(pool.map(lambda x: x.apply(match_article, axis=1), df_split))\n",
    "    return df\n",
    "\n",
    "df_transactions[\"Match\"] = parallel_fuzzy_matching(df_transactions)\n",
    "\n",
    "# 4Ô∏è‚É£ Save optimized result\n",
    "df_transactions.to_csv(\"C:/Users/lgino/code/Lothardill/Hospitalis/Transactions_with_Match.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ Fully optimized fuzzy matching completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69986e-62b2-4d1c-ba73-ea709fee7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voir ave la columne fournisseur, voir le chiffre pas obliqu√© des uniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
